{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "!pip install torch --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "!pip install seaborn --quiet\n",
    "!pip install ipywidgets --quiet\n",
    "!pip install tqdm --quiet\n",
    "!pip install wandb --quiet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4dbdc422-f89a-4706-b1e2-78d3f5240303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from rouge_score import *\n",
    "#from ROUGE_Score_Model import Model\n",
    "from evaluators import METEOR\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torch import nn, Tensor, flatten\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca58ff56-6a4b-4909-a7dc-c66c11fc7943",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.9055])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor = METEOR()\n",
    "meteor('The cat ate the mouse', 'The cat ate the mouse The cat ate the mouse')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "from torch import nn, Tensor, cat, flatten\n",
    "from rouge_score import *\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, summary_len_mean, summary_len_std):\n",
    "        super().__init__()\n",
    "\n",
    "        self.summary_len_mean = summary_len_mean\n",
    "        self.summary_len_std = summary_len_std\n",
    "\n",
    "        input_dim = 2*3 + 1\n",
    "        input_dim = 2\n",
    "\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.non_lin = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, prompt_text, summary_text, scores=None):\n",
    "        if scores is None:\n",
    "            scores = self._calculate_rouge_scores(prompt_text, summary_text)\n",
    "            scores = flatten(scores)\n",
    "        summary_len_norm = self._summary_len_norm(summary_text)\n",
    "\n",
    "        result = cat((scores, summary_len_norm))\n",
    "        result = self.layer1(result)\n",
    "        result = self.non_lin(result)\n",
    "        result = self.layer2(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _calculate_rouge_scores(self, prompt_text, summary_text):\n",
    "\n",
    "        #rouge_cos_1 = rouge_cos_n(prompt_text, summary_text, 1)\n",
    "        #rouge_cos_2 = rouge_cos_n(prompt_text, summary_text, 2)\n",
    "        meteor = METEOR()(prompt_text, summary_text)\n",
    "\n",
    "        scores = [\n",
    "            #(rouge_cos_1.precision, rouge_cos_1.recall, rouge_cos_1.fmeasure),\n",
    "            #(rouge_cos_2.precision, rouge_cos_2.recall, rouge_cos_2.fmeasure),\n",
    "            meteor\n",
    "        ]\n",
    "\n",
    "        return Tensor(scores).to(self.device)\n",
    "\n",
    "    def _summary_len_norm(self, summary_text):\n",
    "        zscore = (len(summary_text) - self.summary_len_mean) / self.summary_len_std\n",
    "        return Tensor((zscore,)).to(self.device)\n",
    "\n",
    "\n",
    "    def to(self, device, *args, **kwargs):\n",
    "        super().to(device, *args, **kwargs)\n",
    "        self.device = device\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "____"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f02c5f1-24fc-4ac3-8363-91f0a78c4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(summary, prompts_df):\n",
    "    return prompts_df.loc[prompts_df.prompt_id == summary.prompt_id].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def preprocess(summaries, prompts):\n",
    "    tqdm.pandas()\n",
    "\n",
    "    prompt_columns = ['prompt_text', 'prompt_title', 'prompt_question']\n",
    "\n",
    "    merged_df = summaries.merge(prompts, 'inner', 'prompt_id')\n",
    "\n",
    "    print(\"ROUGE-Scores are being calculated. Please stand by...\")\n",
    "    print('1/4', end=\"\\r\")\n",
    "    merged_df[['rouge1_precision', 'rouge1_recall', 'rouge1_fmeasure']] = merged_df[['text', 'prompt_text']].progress_apply(lambda row: rouge_n(row.prompt_text, row.text, 1), axis=1, result_type='expand')\n",
    "    print('2/4', end=\"\\r\")\n",
    "    merged_df[['rouge2_precision', 'rouge2_recall', 'rouge2_fmeasure']] = merged_df[['text', 'prompt_text']].progress_apply(lambda row: rouge_n(row.prompt_text, row.text, 2), axis=1, result_type='expand')\n",
    "    print('3/4', end=\"\\r\")\n",
    "    merged_df[['rougeL_precision', 'rougeL_recall', 'rougeL_fmeasure']] = merged_df[['text', 'prompt_text']].progress_apply(lambda row: rouge_l(row.prompt_text, row.text), axis=1, result_type='expand')\n",
    "    print('4/4', end=\"\\r\")\n",
    "    merged_df[['rougeLsum_precision', 'rougeLsum_recall', 'rougeLsum_fmeasure']] = merged_df[['text', 'prompt_text']].progress_apply(lambda row: rouge_lsum(row.prompt_text, row.text), axis=1, result_type='expand')\n",
    "    print(\"Done\")\n",
    "\n",
    "    summaries = merged_df.drop(prompt_columns, axis=1)\n",
    "    return summaries, prompts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def get_scores(row):\n",
    "    scores = []\n",
    "    for score in ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']:\n",
    "        scores.append((row[f'{score}_precision'],\n",
    "                       row[f'{score}_recall'],\n",
    "                       row[f'{score}_fmeasure']))\n",
    "\n",
    "    scores = torch.Tensor(scores)\n",
    "    scores = flatten(scores)\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def predict(model, target_score, summary, prompts_df, scores=None):\n",
    "    prompt = get_prompt(summary, prompts_df)\n",
    "\n",
    "    if target_score == 'content':\n",
    "        target = Tensor([summary.content]).to(device)\n",
    "    elif target_score == 'wording':\n",
    "        target = Tensor([summary.wording]).to(device)\n",
    "    else:\n",
    "        target = Tensor([summary.content, summary.wording]).to(device)\n",
    "\n",
    "    predictions = model(prompt.prompt_text, summary.text, scores)\n",
    "    return predictions, target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def mcrmse(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shapes of y_true and y_pred must be the same.\")\n",
    "    rmse_values = np.sqrt(np.mean((y_true - y_pred)**2, axis=0))\n",
    "    mcrmse = np.mean(rmse_values)\n",
    "\n",
    "    return mcrmse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "bab07629-50e8-43c0-a068-6c55c898afed",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "727844ee-c63d-47cd-b471-da17ae774c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36c44ab7-38b1-4466-a5e0-7c6eb93a7330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;34mGPU not available. CPU used.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "device, path = setup(wandb_key=KEY)\n",
    "summaries_df, prompts_df = get_data('../kaggle/input/commonlit-evaluate-student-summaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41846d8d-7db2-4862-8e91-49c13d976e45",
   "metadata": {},
   "source": [
    "## Set up Data (1)\n",
    "**Stratified Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b2cf556-6d2b-4129-a579-72716412cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(summaries_df, test_size=0.2, stratify=summaries_df[\"prompt_id\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b01f88e-c827-4991-9365-3cb2843952f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len_mean = train_df.text.apply(len).mean()\n",
    "text_len_std  = train_df.text.apply(len).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f83bc426-2369-4033-8700-0abd0d32404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "hidden_dim = 64\n",
    "target_score = 'content'\n",
    "#target_score = 'wording'\n",
    "#target_score = 'both'\n",
    "output_dim = 2 if target_score == 'both' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "726d6806-0a01-4cae-b154-b4eb80b66df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_dim, output_dim, text_len_mean, text_len_std).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1da519af-8a23-4956-995c-1d177cdf80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer=torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "622ca1d7-1f4d-4a2b-959f-0b00e58ca889",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer(list(model.parameters()))\n",
    "optimizer.lr = learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da988fc-2c43-4708-8f8d-495bd1c954e0",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22b44132-1e4c-4b2e-8d26-bc52cceee673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project = \"ESS_4\"\n",
    "name = f\"ROUGE_{target_score}\"\n",
    "#notes = \"Trained on all four topics\"\n",
    "\n",
    "#architecture = \"ROUGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6172159c-a042-4eac-8b44-2e7abf4d3a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'wandb.init(\\n    project=project,\\n    name=name,\\n    notes=notes,\\n\\n    # track hyperparameters and run metadata\\n    config={\\n        \"architecture\": architecture,\\n        \"learning_rate\": learning_rate,\\n        \"epochs\": epochs,\\n        \"loss_function\":type(criterion),\\n        \"optimizer\":type(optimizer),\\n        \"hidden_dim\":hidden_dim,\\n        \"non-lin\":model.non_lin\\n        },\\n)'"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"wandb.init(\n",
    "    project=project,\n",
    "    name=name,\n",
    "    notes=notes,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"architecture\": architecture,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": epochs,\n",
    "        \"loss_function\":type(criterion),\n",
    "        \"optimizer\":type(optimizer),\n",
    "        \"hidden_dim\":hidden_dim,\n",
    "        \"non-lin\":model.non_lin\n",
    "        },\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['student_id', 'prompt_id', 'text', 'content', 'wording'], dtype='object')"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df, prompts_df = preprocess(train_df, prompts_df)\n",
    "train_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3958a9b3-bdcb-4e79-b29d-f963a8056c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e16bc31b77de4a89b9a44689e149316f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for index, summary in train_df.iterrows():\n",
    "        #scores = get_scores(summary)\n",
    "\n",
    "        predictions, target = predict(model, target_score, summary, prompts_df)\n",
    "        loss = criterion(predictions, target)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_true.append([*(float(x) for x in target)])\n",
    "        y_pred.append([*(float(x) for x in predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1147 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef64515fa7194a85b8955e33e3b75c84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for index, summary in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "\n",
    "    predictions, target = predict(model, target_score, summary, prompts_df)\n",
    "\n",
    "    y_true.append([*(float(x) for x in target)])\n",
    "    y_pred.append([*(float(x) for x in predictions)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "97c1f68c-3ced-4e97-b00e-c7a762c77d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCRMSE: 0.5125\n",
      "MSE: 0.2627\n",
      "R2:  0.7774\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"MCRMSE: {mcrmse(y_true, y_pred):.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"R2:  {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34183581-35e1-4895-a86e-7d8040a3f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"models/ROUGE/{name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
